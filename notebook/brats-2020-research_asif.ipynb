{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install monai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:59:35.565374Z","iopub.execute_input":"2025-02-16T11:59:35.565688Z","iopub.status.idle":"2025-02-16T11:59:40.709426Z","shell.execute_reply.started":"2025-02-16T11:59:35.565665Z","shell.execute_reply":"2025-02-16T11:59:40.708603Z"}},"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.10/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.5.1+cu121)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.24->monai) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.24->monai) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9->monai) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0,>=1.24->monai) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.24->monai) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0,>=1.24->monai) (2024.2.0)\nDownloading monai-1.4.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: monai\nSuccessfully installed monai-1.4.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install nibabel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:59:40.710581Z","iopub.execute_input":"2025-02-16T11:59:40.710827Z","iopub.status.idle":"2025-02-16T11:59:44.028622Z","shell.execute_reply.started":"2025-02-16T11:59:40.710797Z","shell.execute_reply":"2025-02-16T11:59:44.027583Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (5.3.2)\nRequirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel) (5.13.0)\nRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from nibabel) (1.26.4)\nRequirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from nibabel) (24.2)\nRequirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from nibabel) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22->nibabel) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22->nibabel) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22->nibabel) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22->nibabel) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22->nibabel) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22->nibabel) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22->nibabel) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22->nibabel) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22->nibabel) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22->nibabel) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22->nibabel) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport matplotlib.pyplot as plt \nimport os\nimport torch\nfrom monai.data import DataLoader, Dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:35:54.857831Z","iopub.execute_input":"2025-02-16T12:35:54.858272Z","iopub.status.idle":"2025-02-16T12:35:54.862552Z","shell.execute_reply.started":"2025-02-16T12:35:54.858224Z","shell.execute_reply":"2025-02-16T12:35:54.861626Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"data_dir = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\nsample_patient = \"BraTS20_Training_001\"\n\nmodalities = {\n    \"FLAIR\": f\"{data_dir}{sample_patient}/{sample_patient}_flair.nii\",\n    \"T1\": f\"{data_dir}{sample_patient}/{sample_patient}_t1.nii\",\n    \"T1ce\": f\"{data_dir}{sample_patient}/{sample_patient}_t1ce.nii\",\n    \"T2\": f\"{data_dir}{sample_patient}/{sample_patient}_t2.nii\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:35:58.638967Z","iopub.execute_input":"2025-02-16T12:35:58.639273Z","iopub.status.idle":"2025-02-16T12:35:58.643499Z","shell.execute_reply.started":"2025-02-16T12:35:58.639250Z","shell.execute_reply":"2025-02-16T12:35:58.642584Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Displaying some sample images","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,4, figsize=(20,10))\nfor i, (modality, path) in enumerate(modalities.items()):\n    print(path)\n    img = nib.load(path).get_fdata()\n\n    mid_slice = img.shape[2] // 2\n\n    axes[i].imshow(img[:, :, mid_slice], cmap=\"gray\", aspect=\"auto\")\n    axes[i].set_title(modality, fontsize=14)\n    axes[i].axis(\"off\")\n\nplt.show()\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the segmentation mask\nseg_path = f\"{data_dir}{sample_patient}/{sample_patient}_seg.nii\"\nseg_img = nib.load(seg_path).get_fdata()\n\n# Plot segmentation mask\nplt.figure(figsize=(6,6))\nplt.imshow(seg_img[:, :, mid_slice], cmap=\"jet\")  # Use \"jet\" colormap for segmentation\nplt.axis(\"off\")\nplt.title(\"Tumor Segmentation Mask\", fontsize=14)\nplt.colorbar()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Viewing 3D","metadata":{}},{"cell_type":"code","source":"flair_path = f\"{data_dir}{sample_patient}/{sample_patient}_t1ce.nii\"\n\nimg = nib.load(flair_path).get_fdata()\n\nimg = (img - np.min(img)) / (np.max(img) - np.min(img))\n\nmid_slice = img.shape[2] // 2  \nalt_slice = img.shape[2] // 3  \n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].imshow(img[:, :, mid_slice], cmap=\"hot\")  \naxes[0].set_title(\"Middle Slice\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(img[:, :, alt_slice], cmap=\"gray\") \naxes[1].set_title(\"Alternative Slice\")\naxes[1].axis(\"off\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.imshow(img[:, :, img.shape[2] // 2], cmap=\"gray\")\nplt.title(\"Middle Slice of FLAIR MRI\")\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.graph_objects as go","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = nib.load(flair_path).get_fdata()\n\nthreshold = np.percentile(img,50)\n\nx,y,z = np.where(img > threshold)\n\nvalues = img[x,y,z]\n\n#downsample\nsample_size = min(5000, len(x))\nindices = np.random.choice(len(x), sample_size, replace=False)\n\n\nx, y, z, values = x[indices], y[indices], z[indices], values[indices]\n\nfig = go.Figure(data=go.Scatter3d(\n    x=x, y=y, z=z,\n    mode=\"markers\",\n    marker=dict(\n        size=3,  \n        color=values,  \n        colorscale=\"hot\", \n        opacity=0.8  \n    )    \n))\n\nfig.update_layout(\n    title=\"3D MRI Scan Visualization (FLAIR)\",\n    scene=dict(\n        xaxis_title=\"X\",\n        yaxis_title=\"Y\",\n        zaxis_title=\"Z\"\n    )\n)\n\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install hd-bet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get the Dataset Ready and Start Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.nn as nn\nimport monai\nimport numpy as np\nimport nibabel as nib\nfrom monai.transforms import (\n    Compose, LoadImage,ScaleIntensity, RandRotated, RandFlipd, RandZoomd,\n    EnsureTyped, Resize\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:24:07.299411Z","iopub.execute_input":"2025-02-16T12:24:07.299797Z","iopub.status.idle":"2025-02-16T12:24:24.937681Z","shell.execute_reply.started":"2025-02-16T12:24:07.299766Z","shell.execute_reply":"2025-02-16T12:24:24.936993Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:24:24.938653Z","iopub.execute_input":"2025-02-16T12:24:24.939238Z","iopub.status.idle":"2025-02-16T12:24:24.953635Z","shell.execute_reply.started":"2025-02-16T12:24:24.939214Z","shell.execute_reply":"2025-02-16T12:24:24.952893Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"base_path = \"/kaggle/input/brats20-dataset-training-validation/\"\ntrain_path = os.path.join(base_path, \"BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\")\nval_path = os.path.join(base_path, \"BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:24:24.955485Z","iopub.execute_input":"2025-02-16T12:24:24.955750Z","iopub.status.idle":"2025-02-16T12:24:24.959253Z","shell.execute_reply.started":"2025-02-16T12:24:24.955729Z","shell.execute_reply":"2025-02-16T12:24:24.958566Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm  # For progress tracking\n\ndef get_file_list(data_path, is_train=True):\n    if not os.path.exists(data_path):\n        print(f\"Path not found: {data_path}\")\n        return []\n\n    patients = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n    print(f\"Found {len(patients)} patients in {data_path}\") \n\n    data = []\n    for patient in tqdm(patients, desc=\"Processing Patients\"):\n        patient_path = os.path.join(data_path, patient)\n        modalities = {\n            't1': os.path.join(patient_path, f\"{patient}_t1.nii\"),\n            't1ce': os.path.join(patient_path, f\"{patient}_t1ce.nii\"),\n            't2': os.path.join(patient_path, f\"{patient}_t2.nii\"),\n            'flair': os.path.join(patient_path, f\"{patient}_flair.nii\"),\n        }\n\n        # Also check for .nii.gz files if .nii files are missing\n        for mod in modalities.keys():\n            if not os.path.exists(modalities[mod]):\n                gz_path = modalities[mod] + \".gz\"\n                if os.path.exists(gz_path):\n                    modalities[mod] = gz_path  # Use .nii.gz instead\n\n        if is_train:\n            modalities['seg'] = os.path.join(patient_path, f\"{patient}_seg.nii\")\n            if not os.path.exists(modalities['seg']):\n                seg_gz_path = modalities['seg'] + \".gz\"\n                if os.path.exists(seg_gz_path):\n                    modalities['seg'] = seg_gz_path\n\n        # Check if any modality is still missing\n        missing_files = [mod for mod, path in modalities.items() if not os.path.exists(path)]\n        if missing_files:\n            print(f\"Skipping {patient} (Missing: {', '.join(missing_files)})\")\n            continue  \n\n        data.append(modalities)\n    \n    print(f\"Total valid patients: {len(data)}\")\n    return data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:24:24.960065Z","iopub.execute_input":"2025-02-16T12:24:24.960345Z","iopub.status.idle":"2025-02-16T12:24:24.978008Z","shell.execute_reply.started":"2025-02-16T12:24:24.960308Z","shell.execute_reply":"2025-02-16T12:24:24.977345Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_data = get_file_list(train_path, is_train=True)\nval_data = get_file_list(val_path, is_train=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:24:24.978804Z","iopub.execute_input":"2025-02-16T12:24:24.978994Z","iopub.status.idle":"2025-02-16T12:24:27.573996Z","shell.execute_reply.started":"2025-02-16T12:24:24.978976Z","shell.execute_reply":"2025-02-16T12:24:27.573212Z"}},"outputs":[{"name":"stdout","text":"Found 369 patients in /kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\n","output_type":"stream"},{"name":"stderr","text":"Processing Patients: 100%|██████████| 369/369 [00:01<00:00, 223.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Skipping BraTS20_Training_355 (Missing: seg)\nTotal valid patients: 368\nFound 125 patients in /kaggle/input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/\n","output_type":"stream"},{"name":"stderr","text":"Processing Patients: 100%|██████████| 125/125 [00:00<00:00, 242.78it/s]","output_type":"stream"},{"name":"stdout","text":"Total valid patients: 125\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(f\"Training samples: {len(train_data)}\")\nprint(f\"Validation samples: {len(val_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:24:27.574723Z","iopub.execute_input":"2025-02-16T12:24:27.574939Z","iopub.status.idle":"2025-02-16T12:24:27.578995Z","shell.execute_reply.started":"2025-02-16T12:24:27.574920Z","shell.execute_reply":"2025-02-16T12:24:27.578043Z"}},"outputs":[{"name":"stdout","text":"Training samples: 368\nValidation samples: 125\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import shutil\n\n# Path to Kaggle working directory\noutput_dir = \"/kaggle/working/preprocessed_brats2020/\"\n\n# Delete existing output directory (if it exists) to free up space\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\n    print(\"Deleted existing output directory.\")\n\n# Recreate the output directory\nos.makedirs(output_dir, exist_ok=True)\nprint(\"Clean workspace ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:30:59.683460Z","iopub.execute_input":"2025-02-16T12:30:59.683850Z","iopub.status.idle":"2025-02-16T12:31:01.493407Z","shell.execute_reply.started":"2025-02-16T12:30:59.683823Z","shell.execute_reply":"2025-02-16T12:31:01.492564Z"}},"outputs":[{"name":"stdout","text":"Deleted existing output directory.\nClean workspace ready!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tqdm import tqdm\nfrom monai.transforms import (\n    Compose, LoadImaged, EnsureChannelFirstd, Spacingd,\n    ScaleIntensityd, RandRotated, RandFlipd, RandZoomd, EnsureTyped\n)\nfrom monai.data import Dataset, DataLoader\n\n# Define Augmentations for Training Data\ntrain_transforms = Compose([\n    LoadImaged(keys=[\"image\", \"label\"]),\n    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n    ScaleIntensityd(keys=['image'], minv=0.0, maxv=1.0),\n    RandRotated(keys=['image'], range_x=15, prob=0.5),\n    RandFlipd(keys=['image'], prob=0.5),\n    RandZoomd(keys=['image'], min_zoom=0.9, max_zoom=1.1, prob=0.5),\n    EnsureTyped(keys=['image', 'label'])\n])\n\n# Transformations for Validation (No Augmentation)\nval_transforms = Compose([\n    LoadImaged(keys=[\"image\", \"label\"]),\n    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n    ScaleIntensityd(keys=['image'], minv=0.0, maxv=1.0),\n    EnsureTyped(keys=['image', 'label'])\n])\n\n# Function to Apply Transformations and Save as .npy\ndef preprocess_and_save(dataset, save_dir, transforms):\n    os.makedirs(save_dir, exist_ok=True)\n\n    for idx, patient_data in enumerate(tqdm(dataset, desc=\"Processing Patients\")):\n        transformed = transforms(patient_data)  # Apply transformations\n\n        np.save(os.path.join(save_dir, f\"patient_{idx}_image.npy\"), transformed[\"image\"])\n        np.save(os.path.join(save_dir, f\"patient_{idx}_label.npy\"), transformed[\"label\"])\n\n# Apply Augmentations & Save (Training with Augmentation, Validation without)\npreprocess_and_save(train_data, save_dir=\"train_preprocessed\", transforms=train_transforms)\npreprocess_and_save(val_data, save_dir=\"val_preprocessed\", transforms=val_transforms)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tqdm import tqdm\nfrom monai.transforms import (\n    Compose, Resize, ScaleIntensity, EnsureChannelFirst,\n    RandRotate, RandFlip, RandZoom\n)\nimport nibabel as nib  # To read NIfTI files\n\n# Define Preprocessing Pipeline (For Validation - No Augmentation)\nval_preprocess = Compose([\n    Resize(spatial_size=(128, 128, 128)),  # Resize to 128x128x128\n    ScaleIntensity(minv=0, maxv=1),  # Normalize intensity\n    EnsureChannelFirst()  # Ensure correct shape (C, H, W, D)\n])\n\n# Define Preprocessing + Augmentation Pipeline (For Training)\ntrain_preprocess = Compose([\n    Resize(spatial_size=(128, 128, 128)),  # Resize to 128x128x128\n    ScaleIntensity(minv=0, maxv=1),  # Normalize intensity\n    EnsureChannelFirst(),  # Ensure correct shape (C, H, W, D)\n    RandRotate(range_x=15, prob=0.5),  # Randomly rotate (±15 degrees)\n    RandFlip(spatial_axis=0, prob=0.5),  # Randomly flip horizontally\n    RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5)  # Random zoom\n])\n\n# Function to extract label from segmentation (.seg) file\ndef get_label(seg_path):\n    seg_img = nib.load(seg_path).get_fdata()  # Load segmentation image\n    return 1 if 4 in seg_img else 0  # HGG if 4 is present, else LGG\n\n# Function to process and save images & labels\ndef process_and_save(data_list, save_dir, transform):\n    os.makedirs(save_dir, exist_ok=True)\n    \n    for patient in tqdm(data_list, desc=f\"Processing {save_dir}\"):\n        patient_id = patient[\"id\"]  # Unique identifier for patient\n        seg_path = patient[\"seg\"]  # Segmentation file path\n        \n        # Extract label from segmentation file\n        label = get_label(seg_path)\n\n        # Apply preprocessing and augmentation (for training)\n        image_stack = np.stack([\n            transform(patient[\"FLAIR\"]),\n            transform(patient[\"T1\"]),\n            transform(patient[\"T1ce\"]),\n            transform(patient[\"T2\"])\n        ], axis=0)  # Shape: (4, 128, 128, 128)\n\n        # Save as NumPy file (image & label separately)\n        np.save(os.path.join(save_dir, f\"{patient_id}_image.npy\"), image_stack)\n        np.save(os.path.join(save_dir, f\"{patient_id}_label.npy\"), np.array(label))\n\n# Process training data with augmentation\nprocess_and_save(train_data, save_dir=\"train_preprocessed\", transform=train_preprocess)\n\n# Process validation data without augmentation\nprocess_and_save(val_data, save_dir=\"val_preprocessed\", transform=val_preprocess)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Checking if Preprocessing is done properly","metadata":{}},{"cell_type":"code","source":"#Checking Numpy Vectors\nimport numpy as np\n\n# Load a preprocessed sample\nsample_image = np.load(\"train_preprocessed/patient_0_0_image.npy\")\n\n# Print details\nprint(\"🔹 NumPy Array Details:\")\nprint(f\" - Shape: {sample_image.shape}\")\nprint(f\" - Data Type: {sample_image.dtype}\")\nprint(f\" - Min Intensity: {sample_image.min():.4f}\")\nprint(f\" - Max Intensity: {sample_image.max():.4f}\")\n\n\"\"\"\nShape: (1, 128, 128, 128)  # Single channel (grayscale MRI)\n - Data Type: float32\n - Min Intensity: 0.0000\n - Max Intensity: 1.0000\n \n \"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Visualizing\nimport matplotlib.pyplot as plt\n\n# Select the middle slice\nmid_slice = sample_image.shape[2] // 2  \n\nplt.imshow(sample_image[0, :, :, mid_slice], cmap=\"gray\")  # Show middle slice\nplt.title(\"Preprocessed MRI - Middle Slice\")\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Doing the Preprocessing for a single Patient","metadata":{}},{"cell_type":"code","source":"#Checking for one patient\n# MONAI Transforms\nload_nifti = LoadImage(image_only=True)\nresize_transform = Resize(spatial_size=(128, 128, 128), mode=\"trilinear\")\nnormalize_transform = ScaleIntensity(minv=0, maxv=1)\n\n# Select one patient from training data\none_patient = train_data[0]  # First patient in train_data\nprint(f\"Processing Patient: {one_patient}\")\n\n# Preprocess & store results\npreprocessed_images = {}\n\nfor modality, file_path in one_patient.items():\n    if modality == 'seg':  # Ignore segmentation\n        continue\n\n    if not os.path.exists(file_path):\n        print(f\"Warning: File not found {file_path}\")\n        continue\n\n    img = load_nifti(file_path)  # Load NIfTI image\n    img = img[0] if img.shape[0] == 1 else img\n    img = resize_transform(img)  # Resize to (128, 128, 128)\n    img = normalize_transform(img)  # Normalize intensity\n\n    preprocessed_images[modality] = img.numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:42:39.568892Z","iopub.execute_input":"2025-02-16T12:42:39.569208Z","iopub.status.idle":"2025-02-16T12:43:09.228034Z","shell.execute_reply.started":"2025-02-16T12:42:39.569182Z","shell.execute_reply":"2025-02-16T12:43:09.227252Z"}},"outputs":[{"name":"stdout","text":"Processing Patient: {'t1': '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_083/BraTS20_Training_083_t1.nii', 't1ce': '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_083/BraTS20_Training_083_t1ce.nii', 't2': '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_083/BraTS20_Training_083_t2.nii', 'flair': '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_083/BraTS20_Training_083_flair.nii', 'seg': '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_083/BraTS20_Training_083_seg.nii'}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Show NumPy array details\nmodality_to_show = \"flair\"  # Change to 't1', 't1ce', or 't2' if needed\n\nif modality_to_show in preprocessed_images:\n    sample_img = preprocessed_images[modality_to_show]\n    \n    # Print NumPy details\n    print(f\"\\n🔹 NumPy Array Details for {modality_to_show.upper()}:\")\n    print(f\" - Shape: {sample_img.shape}\")\n    print(f\" - Data Type: {sample_img.dtype}\")\n    print(f\" - Min Value: {np.min(sample_img):.4f}\")\n    print(f\" - Max Value: {np.max(sample_img):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:43:09.229422Z","iopub.execute_input":"2025-02-16T12:43:09.229746Z","iopub.status.idle":"2025-02-16T12:43:10.008524Z","shell.execute_reply.started":"2025-02-16T12:43:09.229715Z","shell.execute_reply":"2025-02-16T12:43:10.007747Z"}},"outputs":[{"name":"stdout","text":"\n🔹 NumPy Array Details for FLAIR:\n - Shape: (240, 128, 128, 128)\n - Data Type: float32\n - Min Value: 0.0000\n - Max Value: 1.0000\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"#Visualizing Few Results\n#Setting up function\n\ndef show_samples(samples):\n    fig, ax = plt.subplots(1,4, figsize=(10,20))\n    modalities = ['T1', 'T1ce', 'T2', 'FLAIR']\n    print(\"Image shape:\", sample['image'].shape)\n\n    for i in range(4):\n        ax[i].imshow(sample['image'][i], cmap='gray')\n        ax[i].set_title(f\"{modalities[i]}\\nClass: {'HGG' if sample['label'] else 'LGG'}\")\n        ax[i].axis('off')\n    plt.show()\n\n# Visualize random training sample\ntrain_dataset = BrainTumourDataset(train_data[5:], train_transforms)\nsample = train_dataset[45]\nshow_samples(sample)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(\"GPU Available:\", torch.cuda.is_available())\nprint(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T12:49:41.164370Z","iopub.execute_input":"2025-02-16T12:49:41.164786Z","iopub.status.idle":"2025-02-16T12:49:41.429585Z","shell.execute_reply.started":"2025-02-16T12:49:41.164755Z","shell.execute_reply":"2025-02-16T12:49:41.428809Z"}},"outputs":[{"name":"stdout","text":"GPU Available: True\nGPU Name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Creating Custom Pytorch Dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass GliomaDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        \"\"\"\n        Args:\n            data_dir (str): Directory containing the preprocessed .npy files.\n            transform (callable, optional): Optional transform to apply on images.\n        \"\"\"\n        self.data_dir = data_dir\n        self.transform = transform\n        \n        # Get list of all patient IDs (only unique ones)\n        self.patient_ids = sorted(set([f.split(\"_\")[0] for f in os.listdir(data_dir) if f.endswith(\"_image.npy\")]))\n    \n    def __len__(self):\n        return len(self.patient_ids)\n\n    def __getitem__(self, idx):\n        patient_id = self.patient_ids[idx]\n        \n        # Load image and label\n        image_path = os.path.join(self.data_dir, f\"{patient_id}_image.npy\")\n        label_path = os.path.join(self.data_dir, f\"{patient_id}_label.npy\")\n\n        image = np.load(image_path)  # Shape: (4, 128, 128, 128)\n        label = np.load(label_path)  # 0: LGG, 1: HGG\n        \n        # Convert to PyTorch tensors\n        image = torch.tensor(image, dtype=torch.float32)  # (C, H, W, D)\n        label = torch.tensor(label, dtype=torch.long)  # Scalar label\n\n        # Apply transformations if any\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Define dataset paths\ntrain_dir = \"train_preprocessed\"\nval_dir = \"val_preprocessed\"\n\n# Create Dataset instances\ntrain_dataset = GliomaDataset(train_dir)\nval_dataset = GliomaDataset(val_dir)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3D CNN Model for Glioma Classification","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GliomaCNN(nn.Module):\n    def __init__(self):\n        super(GliomaCNN, self).__init__()\n\n        # Conv Block 1\n        self.conv1 = nn.Conv3d(in_channels=4, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm3d(32)\n        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n\n        # Conv Block 2\n        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm3d(64)\n        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n\n        # Conv Block 3\n        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm3d(128)\n        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n\n        # Fully Connected Layers\n        self.fc1 = nn.Linear(128 * 16 * 16 * 16, 256)  # Adjust based on input size\n        self.fc2 = nn.Linear(256, 2)  # 2 Classes: LGG & HGG\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n\n        x = x.view(x.size(0), -1)  # Flatten\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)  # No activation, since CrossEntropyLoss includes softmax\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining the Loss and Optimizer","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n# Define Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = GliomaCNN().to(device)\n\n# Loss Function & Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"code","source":"num_epochs = 10  # Adjust based on Kaggle runtime\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n\nprint(\"Training Complete!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation of Model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport numpy as np\n\ndef evaluate_model(model, dataloader, device):\n    model.eval()  # Set model to evaluation mode\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():  # No gradient calculation\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)  # Get the class with highest probability\n            \n            all_preds.extend(preds.cpu().numpy())  # Store predictions\n            all_labels.extend(labels.cpu().numpy())  # Store true labels\n\n    # Compute metrics\n    acc = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='binary')\n    recall = recall_score(all_labels, all_preds, average='binary')\n    f1 = f1_score(all_labels, all_preds, average='binary')\n    cm = confusion_matrix(all_labels, all_preds)\n\n    print(f\"🔹 Accuracy: {acc:.4f}\")\n    print(f\"🔹 Precision: {precision:.4f}\")\n    print(f\"🔹 Recall: {recall:.4f}\")\n    print(f\"🔹 F1 Score: {f1:.4f}\")\n    print(\"🔹 Confusion Matrix:\")\n    print(cm)\n\n    return acc, precision, recall, f1, cm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking on Vision Transformer","metadata":{}},{"cell_type":"code","source":"#importing Libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom timm.models.vision_transformer import vit_base_patch16_224\nimport numpy as np\nimport os\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T13:28:08.452756Z","iopub.execute_input":"2025-02-16T13:28:08.453183Z","iopub.status.idle":"2025-02-16T13:28:10.934691Z","shell.execute_reply.started":"2025-02-16T13:28:08.453151Z","shell.execute_reply":"2025-02-16T13:28:10.933987Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Custom PyTorch Dataset for ViT","metadata":{}},{"cell_type":"code","source":"class GliomaViTDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.data_list = [f for f in os.listdir(data_dir) if f.endswith('.npy')]\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, idx):\n        file_path = os.path.join(self.data_dir, self.data_list[idx])\n        data = np.load(file_path, allow_pickle=True).item()\n\n        # Extract the center slice from each modality\n        center_slice = 64  # Middle slice of (128,128,128)\n        flair = data[\"FLAIR\"][:, :, center_slice]\n        t1 = data[\"T1\"][:, :, center_slice]\n        t1ce = data[\"T1ce\"][:, :, center_slice]\n        t2 = data[\"T2\"][:, :, center_slice]\n\n        # Stack as RGB image (ViT requires 3 channels)\n        image = np.stack([flair, t1ce, t2], axis=0)  # Shape: (3, 128, 128)\n\n        label = 1 if np.max(data[\"seg\"]) == 4 else 0  # HGG if label==4 else LGG\n\n        if self.transform:\n            image = self.transform(image)\n\n        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\n# Define Transformations\nvit_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224)),  # Resize to 224x224 for ViT\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load Dataset\ntrain_dataset = GliomaViTDataset(\"train_preprocessed\", transform=vit_transform)\nval_dataset = GliomaViTDataset(\"val_preprocessed\", transform=vit_transform)\n\n# Create Dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define the ViT Model","metadata":{}},{"cell_type":"code","source":"class ViTClassifier(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ViTClassifier, self).__init__()\n        self.vit = vit_base_patch16_224(pretrained=True)\n        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)\n\n    def forward(self, x):\n        return self.vit(x)\n\n# Move model to GPU (if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ViTClassifier().to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train_vit(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n    model.train()\n    \n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n        train_acc = correct / total\n        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f}\")\n\n# Train Model\ntrain_vit(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    acc = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='binary')\n    recall = recall_score(all_labels, all_preds, average='binary')\n    f1 = f1_score(all_labels, all_preds, average='binary')\n    cm = confusion_matrix(all_labels, all_preds)\n\n    print(f\"🔹 Accuracy: {acc:.4f}\")\n    print(f\"🔹 Precision: {precision:.4f}\")\n    print(f\"🔹 Recall: {recall:.4f}\")\n    print(f\"🔹 F1 Score: {f1:.4f}\")\n    print(\"🔹 Confusion Matrix:\")\n    print(cm)\n\n# Run Evaluation\nevaluate_model(model, val_loader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}